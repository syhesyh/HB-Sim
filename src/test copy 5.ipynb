{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class MoE_Activation_Stat:\n",
    "\n",
    "    def __init__(self, stage, n_input_tokens, n_experts=1, experts_per_token=1):\n",
    "        self.stage = stage\n",
    "        self.n_input_tokens = n_input_tokens\n",
    "        self.n_experts = n_experts\n",
    "        self.experts_per_token = experts_per_token\n",
    "        self.expert_token_count = {}  # 存储每个专家被激活的token数量 {expert_id: token_count}\n",
    "        \n",
    "    def get_activated_experts(self, seed=None):\n",
    "        \"\"\"\n",
    "        计算每个token随机激活experts_per_token个专家后，返回每个激活专家及其被激活的token数量\n",
    "        \n",
    "        Args:\n",
    "            seed: 随机种子，用于可重复性\n",
    "            \n",
    "        Returns:\n",
    "            expert_token_count: 字典，{expert_id: token_count}，表示每个激活的专家被多少个token激活\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        \n",
    "        self.expert_token_count.clear()\n",
    "        \n",
    "        # 为每个token随机选择experts_per_token个专家\n",
    "        for token_id in range(self.n_input_tokens):\n",
    "            # 从n_experts个专家中随机选择experts_per_token个（不重复）\n",
    "            selected_experts = random.sample(range(self.n_experts), self.experts_per_token)\n",
    "            # 统计每个专家被多少个token激活\n",
    "            for expert_id in selected_experts:\n",
    "                self.expert_token_count[expert_id] = self.expert_token_count.get(expert_id, 0) + 1\n",
    "        \n",
    "        # 返回每个激活专家及其被激活的token数量\n",
    "        return self.expert_token_count.copy()\n",
    "moe=MoE_Activation_Stat(1,12,12,2).get_activated_experts()\n",
    "print(moe)\n",
    "for token_count, expert_id in moe.items():\n",
    "    print(token_count, expert_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simulate_power_law_activation(n_numbers=600, n_activate=10, alpha=1.0, n_simulations=2000):\n",
    "    \"\"\"\n",
    "    模拟幂律分布激活\n",
    "    \n",
    "    参数:\n",
    "    - n_numbers: 总数\n",
    "    - n_activate: 每次激活数量\n",
    "    - alpha: 幂律指数 (越大，分布越不均匀)\n",
    "    - n_simulations: 模拟次数\n",
    "    \"\"\"\n",
    "    # 生成幂律分布的概率\n",
    "    indices = np.arange(1, n_numbers + 1)\n",
    "    probabilities = indices ** (-alpha)\n",
    "    probabilities = probabilities / probabilities.sum()  # 归一化\n",
    "    \n",
    "    results = []\n",
    "    activation_counts = np.zeros(n_numbers)\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # 不放回抽样，模拟每次激活\n",
    "        activated = np.random.choice(\n",
    "            n_numbers, \n",
    "            size=n_activate, \n",
    "            replace=False, \n",
    "            p=probabilities\n",
    "        )\n",
    "        results.append(activated)\n",
    "        activation_counts[activated] += 1\n",
    "    \n",
    "    return results, activation_counts, probabilities\n",
    "\n",
    "def simulate_zipf_activation(n_numbers=600, n_activate=10, s=1.001, n_simulations=2000):\n",
    "    \"\"\"\n",
    "    使用Zipf分布模拟激活\n",
    "    Zipf分布更符合自然界的幂律现象\n",
    "    \"\"\"\n",
    "    # 生成Zipf分布\n",
    "    # a = np.random.zipf(s, 100000)  # 生成大量样本\n",
    "    # a = a[a <= n_numbers]  # 过滤超出范围的\n",
    "    \n",
    "    # # 估计概率分布\n",
    "    # unique, counts = np.unique(a, return_counts=True)\n",
    "    # probabilities = np.zeros(n_numbers)\n",
    "    # probabilities[unique-1] = counts[:n_numbers] / counts.sum()\n",
    "    \n",
    "    # 如果有些位置概率为0，给一个很小的概率\n",
    "    # zero_mask = probabilities == 0\n",
    "    # probabilities[zero_mask] = 1e-8\n",
    "    # probabilities = probabilities / probabilities.sum()\n",
    "    from scipy.stats import zipf\n",
    "    x = np.arange(1, n_numbers+1)\n",
    "    probabilities = zipf.pmf(x, s)\n",
    "    print(len(probabilities))\n",
    "    probabilities = probabilities / probabilities.sum()\n",
    "    results = []\n",
    "    activation_counts = np.zeros(n_numbers)\n",
    "    for _ in range(n_simulations):\n",
    "        activated = np.random.choice(\n",
    "            n_numbers, \n",
    "            size=n_activate, \n",
    "            replace=False, \n",
    "            p=probabilities\n",
    "        )\n",
    "        results.append(activated)\n",
    "        activation_counts[activated] += 1\n",
    "    \n",
    "    return results, activation_counts, probabilities\n",
    "\n",
    "# 使用Zipf分布\n",
    "zipf_results, zipf_activation_counts, zipf_probs = simulate_zipf_activation()\n",
    "\n",
    "# 使用示例\n",
    "results, counts, probs = simulate_power_law_activation()\n",
    "\n",
    "# 可视化结果\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(probs)\n",
    "plt.title('activation probability distribution')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('probability')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(600), counts[:600])  # 显示前50个的激活次数\n",
    "plt.title('top 50 activation counts')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('activation counts')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(zipf_probs)\n",
    "plt.title('activation zipf probability distribution')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('probability')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(600), zipf_activation_counts[:600])  # 显示前50个的激活次数\n",
    "plt.title('top 50 activation counts')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('activation counts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8501719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request_SpAt_stat():\n",
    "\n",
    "    def __init__(self, n_kv_head, n_cluster, n_layer, prob_func=\"power_law\"):\n",
    "        self.alpha = 1.0\n",
    "        self.s = 1.1\n",
    "        self.n_kv_head = n_kv_head\n",
    "        self.n_cluster = n_cluster\n",
    "        self.n_layer = n_layer\n",
    "        self.activated_prob_table=[[[0 for _ in range(n_cluster)] for _ in range(n_kv_head)] for _ in range(n_layer)]\n",
    "        \n",
    "        for layer in range(n_layer):\n",
    "            for kv_head in range(n_kv_head):\n",
    "                if prob_func == \"power_law\":\n",
    "                    indices = np.arange(1, n_cluster + 1)\n",
    "                    probabilities = indices ** (-self.alpha)\n",
    "                    probabilities = probabilities / probabilities.sum()  # 归一化\n",
    "                    self.activated_prob_table[layer][kv_head] = probabilities\n",
    "                elif prob_func == \"zipf\":\n",
    "                    a = np.random.zipf(self.s, 100000)  # 生成大量样本\n",
    "                    a = a[a <= n_cluster]  # 过滤超出范围的\n",
    "                    \n",
    "                    # 估计概率分布\n",
    "                    unique, counts = np.unique(a, return_counts=True)\n",
    "                    probabilities = np.zeros(n_cluster)\n",
    "                    probabilities[unique-1] = counts[:n_cluster] / counts.sum()\n",
    "                    self.activated_prob_table[layer][kv_head] = probabilities\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid probability function: {prob_func}\")\n",
    "        \n",
    "    def get_activated_prob(self, layer, kv_head, n_avaiable_clusters):\n",
    "        # 已存在的cluster归一化\n",
    "        return self.activated_prob_table[layer][kv_head][:n_avaiable_clusters]/self.activated_prob_table[layer][kv_head][:n_avaiable_clusters].sum()\n",
    "\n",
    "Request_SpAt_stat(2,16,4).get_activated_prob(0,0,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define models and layer.\n",
    "## Generate models\n",
    "import sys\n",
    "import os\n",
    "# 添加项目根目录到Python路径\n",
    "# 如果notebook在src目录下，项目根目录是上一级目录\n",
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith('src'):\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "else:\n",
    "    # 如果不在src目录，假设当前目录就是项目根目录\n",
    "    project_root = current_dir\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.type import *\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FC_Layer:\n",
    "\n",
    "    def __init__(self, stage, name, type, parallel_type, parallel_degree, m, n, k):\n",
    "        self.stage = stage\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        if parallel_type == 'tensor_row':\n",
    "            self.m = m \n",
    "            self.n = n \n",
    "            self.k = k // parallel_degree\n",
    "        elif parallel_type == 'tensor_col':\n",
    "            self.m = m \n",
    "            self.n = n // parallel_degree\n",
    "            self.k = k\n",
    "        else:\n",
    "            self.m = m \n",
    "            self.n = n \n",
    "            self.k = k\n",
    "        self.dbyte = 2 #bf16\n",
    "\n",
    "    def get_infos(self):\n",
    "        return self.m, self.n, self.k\n",
    "\n",
    "    def get_flops(self):\n",
    "        if self.type == LayerType.SOFTMAX or self.type == LayerType.NORM:\n",
    "            return 5 * self.m * self.n\n",
    "\n",
    "        elif self.type == LayerType.ACT:\n",
    "            if 'relu' in self.name:\n",
    "                return 1 * self.m * self.n\n",
    "            elif 'glu' in self.name:\n",
    "                return (8 + 1) * self.m * self.n\n",
    "            else:\n",
    "                return 8 * self.m * self.n\n",
    "        elif self.type == LayerType.FC:\n",
    "            return 2 * self.m * self.n * self.k\n",
    "        elif self.type == LayerType.MATMUL:\n",
    "            return 2 * self.m * self.n\n",
    "        else:\n",
    "            assert 0, \"In Function \\\"get_flops\\\": Not support layer type\"\n",
    "\n",
    "    def get_size(self):\n",
    "        in1 = self.dbyte * self.m * self.n\n",
    "        in2 = self.dbyte * self.n * self.k\n",
    "        out = self.dbyte * self.m * self.k\n",
    "\n",
    "        if self.type in [ LayerType.SOFTMAX, LayerType.NORM,LayerType.ACT, LayerType.MATMUL]:\n",
    "            in1 = self.dbyte * self.m * self.n\n",
    "            in2 = 0\n",
    "            out = in1\n",
    "\n",
    "            # For SwiGLU and GeGLU\n",
    "            if 'glu' in self.name:\n",
    "                in2 = in1\n",
    "\n",
    "        # elif self.type == LayerType.NORM:\n",
    "        #     in1 = self.numOp * self.m * self.n * self.dbyte\n",
    "        #     in2 = in1\n",
    "        #     out = in1\n",
    "\n",
    "        return in1, in2, out\n",
    "\n",
    "\n",
    "class Attention_Layer:\n",
    "\n",
    "    def __init__(self, stage, name, type, parallel_type, parallel_degree, n_q_head, n_kv_head, head_dim, cluster_size, n_cluster, n_activated_clusters):\n",
    "        self.stage = stage\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        self.head_dim = head_dim\n",
    "        self.cluster_size = cluster_size\n",
    "        self.n_cluster = n_cluster\n",
    "        self.n_activated_clusters = n_activated_clusters\n",
    "        self.n_q_head = n_q_head\n",
    "        self.n_kv_head = n_kv_head\n",
    "        self.q_head_per_kv_head = n_q_head // n_kv_head\n",
    "        self.device_per_head = parallel_degree // n_kv_head\n",
    "        self.dbyte = 2\n",
    "        \n",
    "    def get_infos0(self):\n",
    "        if self.type == \"similarity\":\n",
    "            self.m = self.q_head_per_kv_head\n",
    "            self.n = self.head_dim\n",
    "            self.k = self.n_cluster * self.head_dim\n",
    "        elif self.type == \"score\":\n",
    "            self.m = self.q_head_per_kv_head\n",
    "            self.n = self.head_dim\n",
    "            self.k = self.n_activated_clusters * self.cluster_size * self.head_dim // self.device_per_head\n",
    "        elif self.type == \"softmax\":\n",
    "            self.m = self.q_head_per_kv_head\n",
    "            self.n = self.head_dim\n",
    "            self.k = 1 # for softmax, we only need to compute the softmax of the score\n",
    "        elif self.type == \"context\":\n",
    "            self.m = self.q_head_per_kv_head\n",
    "            self.n = self.n_activated_clusters * self.cluster_size * self.head_dim // self.device_per_head\n",
    "            self.k = self.head_dim\n",
    "\n",
    "    def get_infos(self):\n",
    "        return self.m, self.n, self.k\n",
    "\n",
    "    def get_flops(self):\n",
    "        if self.type == \"softmax\":\n",
    "            return 5*self.m*self.n\n",
    "        else:\n",
    "            return 2*self.m*self.n*self.k\n",
    "\n",
    "    def get_size(self):\n",
    "        in1 = self.dbyte * self.m * self.n\n",
    "        in2 = self.dbyte * self.n * self.k\n",
    "        out = self.dbyte * self.m * self.k\n",
    "\n",
    "        if self.type == \"softmax\":\n",
    "            in1 = self.dbyte * self.m * self.n\n",
    "            in2 = 0\n",
    "            out = in1\n",
    "\n",
    "        return in1, in2, out\n",
    "\n",
    "class MoE_Activation_Stat:\n",
    "\n",
    "    def __init__(self, stage, n_input_tokens, n_experts=1, experts_per_token=1):\n",
    "        self.stage = stage\n",
    "        self.n_input_tokens = n_input_tokens\n",
    "        self.n_experts = n_experts\n",
    "        self.experts_per_token = experts_per_token\n",
    "        self.expert_token_count = {}  # 存储每个专家被激活的token数量 {expert_id: token_count}\n",
    "        \n",
    "    def get_activated_experts(self, seed=None):\n",
    "        \"\"\"\n",
    "        计算每个token随机激活experts_per_token个专家后，返回每个激活专家及其被激活的token数量\n",
    "        \n",
    "        Args:\n",
    "            seed: 随机种子，用于可重复性\n",
    "            \n",
    "        Returns:\n",
    "            expert_token_count: 字典，{expert_id: token_count}，表示每个激活的专家被多少个token激活\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        \n",
    "        self.expert_token_count.clear()\n",
    "        \n",
    "        # 为每个token随机选择experts_per_token个专家\n",
    "        for token_id in range(self.n_input_tokens):\n",
    "            # 从n_experts个专家中随机选择experts_per_token个（不重复）\n",
    "            selected_experts = random.sample(range(self.n_experts), self.experts_per_token)\n",
    "            # 统计每个专家被多少个token激活\n",
    "            for expert_id in selected_experts:\n",
    "                self.expert_token_count[expert_id] = self.expert_token_count.get(expert_id, 0) + 1\n",
    "        \n",
    "        # 返回每个激活专家及其被激活的token数量\n",
    "        return self.expert_token_count.copy()\n",
    "\n",
    "class Communication:\n",
    "\n",
    "    def __init__(self, stage, type, parallel_degree, data_size, archi=\"GPU\"):\n",
    "        self.stage = stage\n",
    "        self.data_size = data_size\n",
    "        self.type = type\n",
    "        # self.parallel_type = parallel_type\n",
    "        self.parallel_degree = parallel_degree\n",
    "\n",
    "    def get_infos(self):\n",
    "        return self.stage, self.type, self.parallel_degree, self.data_size\n",
    "\n",
    "    def get_flops(self):\n",
    "        if self.type == CommType.ALL_GATHER:\n",
    "            return 0\n",
    "        elif self.type == CommType.ALL_REDUCE:\n",
    "            return self.data_size * (self.parallel_degree -1)\n",
    "        else:\n",
    "            assert 0, \"not support comm operator type\"\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.data_size * self.parallel_degree\n",
    "\n",
    "class Transformer:\n",
    "\n",
    "    def __init__(self, modelinfos, requestinfos, tensor_parallel=8):\n",
    "        self.tensor_parallel = tensor_parallel\n",
    "        self.decoder_blocks = []\n",
    "        self.name = modelinfos['name']\n",
    "        self.layers = modelinfos['layers']\n",
    "        self.n_experts = modelinfos['n_experts']\n",
    "        self.experts_per_token = modelinfos['experts_per_token']\n",
    "        self.n_q_head = modelinfos['q_head']\n",
    "        self.n_kv_head = modelinfos['kv_head']\n",
    "        self.dhead = modelinfos['dhead']\n",
    "        self.dim = modelinfos['dim']\n",
    "        self.hdim = modelinfos['hdim']\n",
    "        self.hdim_moe = modelinfos['hdim']\n",
    "        self.cluster_size = modelinfos['cluster_size']\n",
    "        self.requestinfos = requestinfos\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        batch = len(self.requestinfos.keys())\n",
    "        decoder_blocks = []\n",
    "        ## QKV\n",
    "        decoder_blocks.append(FC_Layer('decoder', 'qkv', LayerType.FC, \"tensor_col\", self.tensor_parallel, batch, self.dim, self.dhead*(self.n_q_head+2*self.n_kv_head)))\n",
    "\n",
    "        #SpAt\n",
    "        for request_id in self.requestinfos.keys():\n",
    "            for kv_head_id in range(self.n_kv_head):\n",
    "                decoder_blocks.append(Attention_Layer('decoder', 'spat', \"similarity\", None, self.tensor_parallel, \\\n",
    "                    self.n_q_head, self.n_kv_head, self.dhead, self.cluster_size, self.requestinfos[request_id][\"n_cluster\"], self.requestinfos[request_id][\"n_activated_clusters\"]))\n",
    "                decoder_blocks.append(Attention_Layer('decoder', 'spat', \"score\", None, self.tensor_parallel, \\\n",
    "                    self.n_q_head, self.n_kv_head, self.dhead, self.cluster_size, self.requestinfos[request_id][\"n_cluster\"], self.requestinfos[request_id][\"n_activated_clusters\"]))  \n",
    "                decoder_blocks.append(Attention_Layer('decoder', 'spat', \"softmax\", None, self.tensor_parallel, \\\n",
    "                    self.n_q_head, self.n_kv_head, self.dhead, self.cluster_size, self.requestinfos[request_id][\"n_cluster\"], self.requestinfos[request_id][\"n_activated_clusters\"]))\n",
    "                decoder_blocks.append(Attention_Layer('decoder', 'spat', \"context\", None, self.tensor_parallel, \\\n",
    "                    self.n_q_head, self.n_kv_head, self.dhead, self.cluster_size, self.requestinfos[request_id][\"n_cluster\"], self.requestinfos[request_id][\"n_activated_clusters\"]))\n",
    "                \n",
    "        #Proj\n",
    "        decoder_blocks.append(FC_Layer('decoder', 'proj', LayerType.FC, \"tensor_row\", self.tensor_parallel, batch, self.dim, self.dim))\n",
    "        decoder_blocks.append(Communication('decoder', CommType.ALL_REDUCE, self.tensor_parallel, self.dim))\n",
    "\n",
    "        #Attn Norm\n",
    "        decoder_blocks.append(FC_Layer('decoder', 'norm_attn', LayerType.NORM, None, self.tensor_parallel, batch, self.dim, 0))\n",
    "\n",
    "        #MoE Router\n",
    "        decoder_blocks.append(FC_Layer('decoder', 'moe_router', LayerType.FC, \"tensor\", 1, batch, self.dim, self.n_experts))\n",
    "        moe_activation_stat = MoE_Activation_Stat(batch, self.n_experts, self.experts_per_token).get_activated_experts()\n",
    "\n",
    "        #MoE FFN\n",
    "        for expert_id, token_count in moe_activation_stat.items():\n",
    "            decoder_blocks.append(FC_Layer('decoder', f'expert_{expert_id}_moe_gate', LayerType.FC, \"expert\", self.tensor_parallel, token_count, self.dim, self.hdim_moe))\n",
    "            decoder_blocks.append(FC_Layer('decoder', f'expert_{expert_id}_moe_act', LayerType.ACT, None, self.tensor_parallel, token_count, self.dim, self.hdim_moe))\n",
    "            decoder_blocks.append(FC_Layer('decoder', f'expert_{expert_id}_moe_up', LayerType.FC, \"expert\", self.tensor_parallel, token_count, self.dim, self.hdim_moe))\n",
    "            decoder_blocks.append(FC_Layer('decoder', f'expert_{expert_id}_moe_mat', LayerType.MATMUL, \"expert\", self.tensor_parallel, token_count, self.hdim_moe, 0))\n",
    "            decoder_blocks.append(FC_Layer('decoder', f'expert_{expert_id}_moe_down', LayerType.FC, \"expert\", self.tensor_parallel, token_count, self.hdim_moe, self.dim))\n",
    "        decoder_blocks.append(Communication('decoder', CommType.ALL_GATHER, self.tensor_parallel, self.dim))\n",
    "\n",
    "        #MoE Norm\n",
    "        decoder_blocks.append(FC_Layer('decoder', 'moe_attn', LayerType.NORM, None, self.tensor_parallel, batch, self.dim, 0))\n",
    "\n",
    "\n",
    "        self.decoder_blocks.append(decoder_blocks)\n",
    "        return decoder_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7563b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model={\"name\":\"llama4\",\n",
    "        \"layers\":12,\n",
    "        \"n_experts\":12,\n",
    "        \"experts_per_token\":1,\n",
    "        \"q_head\":12,\n",
    "        \"dim\":4096,\n",
    "        \"hdim\":4096,\n",
    "        \"hdim_moe\":128,\n",
    "        \"cluster_size\":128,\n",
    "        \"kv_head\":12,\n",
    "        \"dhead\":64,\n",
    "}\n",
    "Request={\"1\":{\"n_cluster\":12,\"n_activated_clusters\":12},\n",
    "        \"2\":{\"n_cluster\":12,\"n_activated_clusters\":12},\n",
    "        \"3\":{\"n_cluster\":12,\"n_activated_clusters\":12},\n",
    "        \"4\":{\"n_cluster\":12,\"n_activated_clusters\":12},\n",
    "        \"5\":{\"n_cluster\":12,\"n_activated_clusters\":12},\n",
    "        \"10\":{\"n_cluster\":12,\"n_activated_clusters\":12},\n",
    "        \"7\":{\"n_cluster\":12,\"n_activated_clusters\":12}}\n",
    "\n",
    "for i in model:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54aabe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define models and layer.\n",
    "## Generate models\n",
    "import sys\n",
    "import os\n",
    "# 添加项目根目录到Python路径\n",
    "# 如果notebook在src目录下，项目根目录是上一级目录\n",
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith('src'):\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "else:\n",
    "    # 如果不在src目录，假设当前目录就是项目根目录\n",
    "    project_root = current_dir\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import src.infra as infra\n",
    "from src.config import *\n",
    "Model={\"name\":\"llama4\",\n",
    "        \"n_block\":12,\n",
    "        \"n_experts\":12,\n",
    "        \"experts_per_token\":1,\n",
    "        \"q_head\":12,\n",
    "        \"dim\":4096,\n",
    "        \"hdim\":4096,\n",
    "        \"hdim_moe\":128,\n",
    "        \"cluster_size\":128,\n",
    "        \"n_kv_head\":12,\n",
    "        \"dhead\":64,\n",
    "}\n",
    "\n",
    "ENERGY_TABLE=[]\n",
    "gpu=make_xpu_config(GPUType.A100a)\n",
    "\n",
    "GPU={\"name\":\"llama4\",\n",
    "        \"NUM_DEVICE\":8,\n",
    "        \"FLOPS_PER_DEVICE\":1800*1000*1000*1000,\n",
    "        \"OFF_MEM_BW_PER_DEVICE\":8*1024*1024*1024*1024,\n",
    "        \"L2_MEM_BW_PER_DEVICE\":8*1024*1024*1024*1024,\n",
    "        \"MEM_CAPACITY_PER_DEVICE\":192*1024*1024*1024,\n",
    "        \"OFFLOADING_RATIO\":0.67,\n",
    "        \"CPU_COMM_BANDWIDTH\":50*1024*1024*1024,\n",
    "        \"ENERGY_TABLE\":ENERGY_TABLE,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "request_batch = infra.Request_Batch(0.01, Model)\n",
    "for i in range(10):\n",
    "    request_batch.append(i, 12, 12)\n",
    "system = infra.System(Model, gpu['GPU'], request_batch=request_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63778940",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPU' object has no attribute 'served_request'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m system\u001b[38;5;241m.\u001b[39msim()\n",
      "File \u001b[0;32m~/HB_simulator/src/infra.py:98\u001b[0m, in \u001b[0;36mSystem.system_setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msystem_setup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhardware_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoe_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_block_setup(moe_enable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_block_setup(moe_enable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/HB_simulator/src/infra.py:91\u001b[0m, in \u001b[0;36mSystem.hardware_setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhardware_setup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m DeviceType\u001b[38;5;241m.\u001b[39mGPU:\n\u001b[0;32m---> 91\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGPU \u001b[38;5;241m=\u001b[39m \u001b[43mGPU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDeviceType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhardware_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m DeviceType\u001b[38;5;241m.\u001b[39mPIM:\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPIM \u001b[38;5;241m=\u001b[39m PIM()\n",
      "File \u001b[0;32m~/HB_simulator/src/evaluate.py:67\u001b[0m, in \u001b[0;36mGPU.__init__\u001b[0;34m(self, name, config, request_batch)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_comm_bandwidth \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU_COMM_BANDWIDTH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU_COMM_BANDWIDTH\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m50\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menergy_table \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENERGY_TABLE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserved_request\u001b[49m\u001b[38;5;241m.\u001b[39mactivated_clusters \u001b[38;5;241m=\u001b[39m request_batch\u001b[38;5;241m.\u001b[39mactivated_clusters\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPU' object has no attribute 'served_request'"
     ]
    }
   ],
   "source": [
    "system.system_setup()\n",
    "system.sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(request_batch.request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "request_batch = infra.Request_Batch(0.1, Model)\n",
    "for i in range(10):\n",
    "    request_batch.append(i, 64, 1024)\n",
    "a=request_batch.gen_activated_clusters(0,0)[0]\n",
    "b=(a > 10).sum()\n",
    "b/0.23"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
